{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNd/EbwSHOtstPF8pz3qZDt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/QuantumQist/Hands_on_RL_Lapan/blob/main/ch5_tabular_learning/ch5_frozenLake.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Adjusted source code to chapter 5 of the book**"
      ],
      "metadata": {
        "id": "x-9qoOpMV7d1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "ol04DGZiu27d"
      },
      "outputs": [],
      "source": [
        "#import gym # FIX BELOW + adjust for libraries available in Colab\n",
        "try:\n",
        "  import gymnasium as gym\n",
        "except:\n",
        "  !pip install gymnasium\n",
        "  import gymnasium as gym\n",
        "import collections\n",
        "#from tensorboardX import SummaryWriter # FIX BELOW\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "# ENV_NAME = \"FrozenLake-v0\" # Environment deprecated, FIX BELOW\n",
        "ENV_NAME = \"FrozenLake-v1\"\n",
        "GAMMA = 0.9\n",
        "TEST_EPISODES = 20\n",
        "# Add reward threshold beyond which training is completed\n",
        "# Original code = 0.8\n",
        "REWARD_THRESHOLD = 0.9"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fixes in the code below**\n",
        "\n",
        "1. `self.env.step(action)` (line 12) returns 5 outputs - [observation (ObsType), reward (SupportsFloat), terminated (bool), truncated (bool), info (dict)]. Original code assumes 4 outputs, we keep first 3 of them.\n",
        "2. `self.env.reset()` (as in line 4) returns a tuple with [observation (ObsType), info (dictionary)]. We will keep only the first element to make it compatible with the rest of the code.\n",
        "\n",
        "The book chapter descripes two classes of RL agent based on\n",
        "* Value iteration\n",
        "* Q-Learning\n",
        "\n",
        "We implement both agents here in a single notebook"
      ],
      "metadata": {
        "id": "9sIr66w1Vk4j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent_V:\n",
        "    \"\"\"\n",
        "    RL agent based on value iteration\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.env = gym.make(ENV_NAME)\n",
        "        self.state = self.env.reset()[0] #Fixed from `self.state = self.env.reset()`\n",
        "        self.rewards = collections.defaultdict(float)\n",
        "        self.transits = collections.defaultdict(collections.Counter)\n",
        "        self.values = collections.defaultdict(float)\n",
        "\n",
        "    def play_n_random_steps(self, count):\n",
        "        for _ in range(count):\n",
        "            action = self.env.action_space.sample()\n",
        "            #new_state, reward, is_done, _ = self.env.step(action)\n",
        "            new_state, reward, is_done, _, _ = self.env.step(action)\n",
        "            self.rewards[(self.state, action, new_state)] = reward\n",
        "            self.transits[(self.state, action)][new_state] += 1\n",
        "            # Fix: in the line below we changed\n",
        "            # `self.env.reset()` -> `self.env.reset()[0]`\n",
        "            self.state = self.env.reset()[0] if is_done else new_state\n",
        "\n",
        "    def calc_action_value(self, state, action):\n",
        "        target_counts = self.transits[(state, action)]\n",
        "        total = sum(target_counts.values())\n",
        "        action_value = 0.0\n",
        "        for tgt_state, count in target_counts.items():\n",
        "            reward = self.rewards[(state, action, tgt_state)]\n",
        "            action_value += (count / total) * (reward + GAMMA * self.values[tgt_state])\n",
        "        return action_value\n",
        "\n",
        "    def select_action(self, state):\n",
        "        best_action, best_value = None, None\n",
        "        for action in range(self.env.action_space.n):\n",
        "            action_value = self.calc_action_value(state, action)\n",
        "            if best_value is None or best_value < action_value:\n",
        "                best_value = action_value\n",
        "                best_action = action\n",
        "        return best_action\n",
        "\n",
        "    def play_episode(self, env):\n",
        "        total_reward = 0.0\n",
        "        state = env.reset()[0] # Fixed from `state = env.reset()`\n",
        "        while True:\n",
        "            action = self.select_action(state)\n",
        "            # new_state, reward, is_done, _ = env.step(action) # Fixed below\n",
        "            new_state, reward, is_done, _, _ = env.step(action)\n",
        "            self.rewards[(state, action, new_state)] = reward\n",
        "            self.transits[(state, action)][new_state] += 1\n",
        "            total_reward += reward\n",
        "            if is_done:\n",
        "                break\n",
        "            state = new_state\n",
        "        return total_reward\n",
        "\n",
        "    def value_iteration(self):\n",
        "        for state in range(self.env.observation_space.n):\n",
        "            state_values = [self.calc_action_value(state, action)\n",
        "                            for action in range(self.env.action_space.n)]\n",
        "            self.values[state] = max(state_values)\n"
      ],
      "metadata": {
        "id": "tmJXb5h6WKb2"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent_Q:\n",
        "    \"\"\"\n",
        "    RL agent based on Q learning\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.env = gym.make(ENV_NAME)\n",
        "        self.state = self.env.reset()[0] #Fixed from `self.state = self.env.reset()`\n",
        "        self.rewards = collections.defaultdict(float)\n",
        "        self.transits = collections.defaultdict(collections.Counter)\n",
        "        self.values = collections.defaultdict(float)\n",
        "\n",
        "    def play_n_random_steps(self, count):\n",
        "        for _ in range(count):\n",
        "            action = self.env.action_space.sample()\n",
        "            #new_state, reward, is_done, _ = self.env.step(action)\n",
        "            new_state, reward, is_done, _, _ = self.env.step(action)\n",
        "            self.rewards[(self.state, action, new_state)] = reward\n",
        "            self.transits[(self.state, action)][new_state] += 1\n",
        "            # Fix: in the line below we changed\n",
        "            # `self.env.reset()` -> `self.env.reset()[0]`\n",
        "            self.state = self.env.reset()[0] if is_done else new_state\n",
        "\n",
        "    def calc_action_value(self, state, action):\n",
        "        target_counts = self.transits[(state, action)]\n",
        "        total = sum(target_counts.values())\n",
        "        action_value = 0.0\n",
        "        for tgt_state, count in target_counts.items():\n",
        "            reward = self.rewards[(state, action, tgt_state)]\n",
        "            action_value += (count / total) * (reward + GAMMA * self.values[tgt_state])\n",
        "        return action_value\n",
        "\n",
        "    def select_action(self, state):\n",
        "        best_action, best_value = None, None\n",
        "        for action in range(self.env.action_space.n):\n",
        "            action_value = self.values[(state, action)]\n",
        "            if best_value is None or best_value < action_value:\n",
        "                best_value = action_value\n",
        "                best_action = action\n",
        "        return best_action\n",
        "\n",
        "    def play_episode(self, env):\n",
        "        total_reward = 0.0\n",
        "        state = env.reset()[0] # Fixed from `state = env.reset()`\n",
        "        while True:\n",
        "            action = self.select_action(state)\n",
        "            # new_state, reward, is_done, _ = env.step(action) # Fixed below\n",
        "            new_state, reward, is_done, _, _ = env.step(action)\n",
        "            self.rewards[(state, action, new_state)] = reward\n",
        "            self.transits[(state, action)][new_state] += 1\n",
        "            total_reward += reward\n",
        "            if is_done:\n",
        "                break\n",
        "            state = new_state\n",
        "        return total_reward\n",
        "\n",
        "    def value_iteration(self):\n",
        "        for state in range(self.env.observation_space.n):\n",
        "            for action in range(self.env.action_space.n):\n",
        "                action_value = 0.0\n",
        "                target_counts = self.transits[(state, action)]\n",
        "                total = sum(target_counts.values())\n",
        "                for tgt_state, count in target_counts.items():\n",
        "                    reward = self.rewards[(state, action, tgt_state)]\n",
        "                    best_action = self.select_action(tgt_state)\n",
        "                    action_value += (count / total) * (reward + GAMMA * self.values[(tgt_state, best_action)])\n",
        "                self.values[(state, action)] = action_value"
      ],
      "metadata": {
        "id": "w3o1Dzs6WMS6"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a function that trains an agent."
      ],
      "metadata": {
        "id": "uQkXDMwdWOjZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_agent(agent_type: str,\n",
        "                env_name: str = ENV_NAME,\n",
        "                reward_threshold: float = REWARD_THRESHOLD):\n",
        "    \"\"\"\n",
        "    Trains an agent.\n",
        "\n",
        "    Parameters:\n",
        "    - agent_type: str\n",
        "        Type of agent. Must be either \"V\" or \"Q\"\n",
        "        V - Value iteration\n",
        "        Q - Q-learning\n",
        "    - env_name: str\n",
        "        Environment name. Example: \"FrozenLake-v1\"\n",
        "    - reward_threshold: float\n",
        "        Reward threshold beyond which training is completed.\n",
        "    The default values for `env_name` and `reward_threshold` are given\n",
        "    in the beginning of the notebook as hyperparameters.\n",
        "\n",
        "    Returns a class with the trained agent.\n",
        "    \"\"\"\n",
        "    # Setup the agent\n",
        "    assert agent_type in [\"V\", \"Q\"], \"`agent` must be either `V` or `Q`\"\n",
        "    agent = Agent_V() if agent_type==\"V\" else Agent_Q()\n",
        "    # Create the envoronment\n",
        "    test_env = gym.make(ENV_NAME)\n",
        "    # Train the agent\n",
        "    iter_no = 0\n",
        "    best_reward = 0.0\n",
        "    while True:\n",
        "        iter_no += 1\n",
        "        agent.play_n_random_steps(100)\n",
        "        agent.value_iteration()\n",
        "\n",
        "        reward = 0.0\n",
        "        for _ in range(TEST_EPISODES):\n",
        "            reward += agent.play_episode(test_env)\n",
        "        reward /= TEST_EPISODES\n",
        "        if reward > best_reward:\n",
        "            print(\"Best reward updated %.3f -> %.3f\" % (best_reward, reward))\n",
        "            best_reward = reward\n",
        "        if reward > reward_threshold:\n",
        "            print(\"Solved in %d iterations!\" % iter_no)\n",
        "            break\n",
        "\n",
        "    test_env.close()\n",
        "\n",
        "    return agent"
      ],
      "metadata": {
        "id": "5Zhtz6dYWR2i"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train both agents**"
      ],
      "metadata": {
        "id": "x2ZYk5xTWT4Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training V-Agent\")\n",
        "v_agent = train_agent(\"V\")\n",
        "print(\"--------------\\nTraining Q-Agent\")\n",
        "q_agent = train_agent(\"Q\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qf-i0C67WYDZ",
        "outputId": "b8219d60-89bf-41d0-ed58-c8d0bf5e9993"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training V-Agent\n",
            "Best reward updated 0.000 -> 0.150\n",
            "Best reward updated 0.150 -> 0.250\n",
            "Best reward updated 0.250 -> 0.400\n",
            "Best reward updated 0.400 -> 0.450\n",
            "Best reward updated 0.450 -> 0.550\n",
            "Best reward updated 0.550 -> 0.700\n",
            "Best reward updated 0.700 -> 0.850\n",
            "Best reward updated 0.850 -> 1.000\n",
            "Solved in 29 iterations!\n",
            "--------------\n",
            "Training Q-Agent\n",
            "Best reward updated 0.000 -> 0.500\n",
            "Best reward updated 0.500 -> 0.550\n",
            "Best reward updated 0.550 -> 0.600\n",
            "Best reward updated 0.600 -> 0.650\n",
            "Best reward updated 0.650 -> 0.800\n",
            "Best reward updated 0.800 -> 0.850\n",
            "Best reward updated 0.850 -> 0.950\n",
            "Solved in 48 iterations!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup a function to visualize the trained agents."
      ],
      "metadata": {
        "id": "GT-7qK8oWaP4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_video(agent, folder_name: str = \"recording\", env_name: str = ENV_NAME):\n",
        "    \"\"\"\n",
        "    Creates a video of a working trained agent.\n",
        "\n",
        "    Parameters:\n",
        "    - agent\n",
        "        Class with the trained agent\n",
        "    - folder_name: str, default value is \"recording\"\n",
        "        Folder where the video will be saved\n",
        "    - env_name: str\n",
        "        Environment used to evaluate the agent.\n",
        "        Default value is specified as hyperparameter in the beginning\n",
        "        of the notebook.\n",
        "\n",
        "    Returns nothing.\n",
        "    \"\"\"\n",
        "    # Setup the environment\n",
        "    video_env = gym.make(env_name, render_mode=\"rgb_array\")\n",
        "    video_env = gym.wrappers.RecordVideo(video_env, folder_name)\n",
        "    # Make the recording\n",
        "    video_env.reset()[0]\n",
        "    video_env.start_video_recorder()\n",
        "    agent.play_episode(env = video_env)\n",
        "    # Close the recording and environment\n",
        "    video_env.close_video_recorder()\n",
        "    video_env.close()"
      ],
      "metadata": {
        "id": "56ER33XPWht4"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create videos of both agents."
      ],
      "metadata": {
        "id": "MNC54It7WjsB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Crating videos:\\nV-Agent:\")\n",
        "create_video(agent = v_agent, folder_name = \"recording_v_agent\", env_name = ENV_NAME)\n",
        "print(\"-----------\\nQ-Agent:\")\n",
        "create_video(agent = q_agent, folder_name = \"recording_q_agent\", env_name = ENV_NAME)\n",
        "print(\"Finished creating videos!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z56MxVQTWmfI",
        "outputId": "896aedcb-9794-4c1f-8f97-5d607d2cf13f"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Crating videos:\n",
            "V-Agent:\n",
            "Moviepy - Building video /content/recording_v_agent/rl-video-episode-0.mp4.\n",
            "Moviepy - Writing video /content/recording_v_agent/rl-video-episode-0.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/recording_v_agent/rl-video-episode-0.mp4\n",
            "Moviepy - Building video /content/recording_v_agent/rl-video-episode-0.mp4.\n",
            "Moviepy - Writing video /content/recording_v_agent/rl-video-episode-0.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/recording_v_agent/rl-video-episode-0.mp4\n",
            "-----------\n",
            "Q-Agent:\n",
            "Moviepy - Building video /content/recording_q_agent/rl-video-episode-0.mp4.\n",
            "Moviepy - Writing video /content/recording_q_agent/rl-video-episode-0.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/recording_q_agent/rl-video-episode-0.mp4\n",
            "Moviepy - Building video /content/recording_q_agent/rl-video-episode-0.mp4.\n",
            "Moviepy - Writing video /content/recording_q_agent/rl-video-episode-0.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                   "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/recording_q_agent/rl-video-episode-0.mp4\n",
            "Finished creating videos!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jouRrGvwWpbh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}